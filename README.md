# dlt-dropbox

## Motivation 

In order to truly democratize Data + AI there needs to be an easy way to ingest files from any source into the bronze and silver layers of the Lakehouse.  Analytics teams outside of typical IT organizations, or downstream of Data Management teams often struggle to get IT resources to set up all of the appropriate cloud data storage containers and buckets, file watchers and file movement processes, let alone additional Unity Catalog external locations and Volumes, needed for each additional file source that needs to be ingested.  Typically, a team might be given a single "landing container" on their cloud data storage and this landing container is then used to move all files from on prem (or outside sources) to an area where the team can use it for analysis.  

![images/DLT-Dropbox File Movement Process.png](https://github.com/matthew-gigl-db/dlt-dropbox/blob/main/images/DLT-Dropbox%20File%20Movement%20Process.png)

With this in mind, we've designed DLT-Dropbox.  A meta-data driven example of how files can be landing into a single container for ingestion and then autoloaded using Delta Live Tables into bronze and silver layer Delta Lake tables.  

## Conceptual Design 

![images/DLT-Dropbox Conceptual Design.png](https://github.com/matthew-gigl-db/dlt-dropbox/blob/main/images/DLT-Dropbox%20Conceptual%20Design.png)




The 'dlt-dropbox' project was generated by using the Databricks Asset Bundles default-python template.

## Getting started with Databricks Asset Bundles

1. Install the Databricks CLI from https://docs.databricks.com/dev-tools/cli/databricks-cli.html

2. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

3. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] dlt_dropbox_job` to your workspace.
    You can find that job by opening your workpace and clicking on **Workflows**.

4. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

   Note that the default job from the template has a schedule that runs every day
   (defined in resources/dlt_dropbox_job.yml). The schedule
   is paused when deploying in development mode (see
   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

5. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```

6. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
   https://docs.databricks.com/dev-tools/vscode-ext.html.

7. For documentation on the Databricks asset bundles format used
   for this project, and for CI/CD configuration, see
   https://docs.databricks.com/dev-tools/bundles/index.html.
